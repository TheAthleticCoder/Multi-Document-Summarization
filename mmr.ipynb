{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cleanData(sentence):\n",
    "\t#sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n",
    "\t#sentence filter(None, re.split(\"[.!?\", setence))\n",
    "\tret = []\n",
    "\tsentence = stemmer.stem(sentence)\t\n",
    "\tfor word in sentence.split():\n",
    "\t\tret.append(word)\n",
    "\treturn \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorSpace(cleanSet):\n",
    "\tvocab = {}\n",
    "\tfor data in cleanSet:\n",
    "\t\tfor word in data.split():\n",
    "\t\t\tvocab[data] = 0\n",
    "\treturn vocab.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSimilarity(sentence, doc):\n",
    "\tif doc == []:\n",
    "\t\treturn 0\n",
    "\tvocab = {}\n",
    "\tfor word in sentence:\n",
    "\t\tvocab[word] = 0\n",
    "\t\n",
    "\tdocInOneSentence = '';\n",
    "\tfor t in doc:\n",
    "\t\tdocInOneSentence += (t + ' ')\n",
    "\t\tfor word in t.split():\n",
    "\t\t\tvocab[word]=0\t\n",
    "\t\n",
    "\tcv = CountVectorizer(vocabulary=vocab.keys())\n",
    "\n",
    "\tdocVector = cv.fit_transform([docInOneSentence])\n",
    "\tsentenceVector = cv.fit_transform([sentence])\n",
    "\treturn cosine_similarity(docVector, sentenceVector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x):\n",
    "    # print(len(x), len(x[:-2]))\n",
    "    x = ' '.join(x[:-2])\n",
    "    x = x.split('\\n')\n",
    "    x = list(filter(lambda s: not s == ' ', x))\n",
    "    x = list(map(lambda s: s.strip(), x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(texts, sentences, clean, originalSentenceOf):\n",
    "    for line in texts:\n",
    "        parts = line.split('.')\n",
    "        for part in parts:\n",
    "            cl = cleanData(part)\n",
    "            sentences.append(part)\n",
    "            clean.append(cl)\n",
    "            originalSentenceOf[cl] = part\t\t\n",
    "    setClean = set(clean)\n",
    "\n",
    "    return setClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "def get_mmr(doc):\n",
    "\talpha = 0.8\n",
    "\tsentences = []\n",
    "\tclean = []\n",
    "\toriginalSentenceOf = {}\n",
    "\n",
    "\tcleanSet = get_sentences(doc, sentences, clean, originalSentenceOf)\n",
    "\n",
    "\tscores = {}\n",
    "\tfor data in clean:\n",
    "\t\ttemp_doc = cleanSet - set([data])\n",
    "\t\tscore = calculateSimilarity(data, list(temp_doc))\n",
    "\t\tscores[data] = score\n",
    "\n",
    "\tn = 20 * len(sentences) / 100\n",
    "\tsummarySet = []\n",
    "\twhile n > 0:\n",
    "\t\tmmr = {}\n",
    "\t\tfor sentence in scores.keys():\n",
    "\t\t\tif not sentence in summarySet:\n",
    "\t\t\t\tmmr[sentence] = alpha * scores[sentence] - (1-alpha) * calculateSimilarity(sentence, summarySet)\t\n",
    "\t\tif mmr == {}:\n",
    "\t\t\tbreak\n",
    "\t\tselected = max(mmr.items(), key=operator.itemgetter(1))[0]\t\n",
    "\t\tsummarySet.append(selected)\n",
    "\t\tn -= 1\n",
    "\n",
    "\toriginal = [originalSentenceOf[sentence].strip() for sentence in summarySet]\n",
    "\t# print ('\\nSummary:\\n')\n",
    "\t# for sentence in summarySet:\n",
    "\t# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "\t# print()\n",
    "\n",
    "\t# print ('=============================================================')\n",
    "\t# print ('\\nOriginal Passages:\\n')\n",
    "\n",
    "\t# for sentence in clean:\n",
    "\t# \tif sentence in summarySet:\n",
    "\t# \t\tprint (colored(originalSentenceOf[sentence].lstrip(' '), 'red'))\n",
    "\t# \telse:\n",
    "\t# \t\tprint (originalSentenceOf[sentence].lstrip(' '))\n",
    "\t\n",
    "\treturn original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2420.15it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 10705.11it/s]\n",
      "100%|██████████| 10/10 [02:03<00:00, 12.33s/it]\n",
      "100%|██████████| 250/250 [00:00<00:00, 3037.86it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 21459.51it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 3710.07it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 18316.70it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "dir_path = 'multi_news/'\n",
    "files = ['sample_train.csv', 'sample_validation.csv', 'sample_test.csv']\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(dir_path, file))\n",
    "    df['documents'] = df['documents'].progress_apply(lambda x: eval(x))\n",
    "    df['concat_doc'] = df['documents'].progress_apply(lambda x: concat(x))\n",
    "    df['mmr'] = df['concat_doc'].progress_apply(lambda x: get_mmr(x))\n",
    "\n",
    "    # drop columns\n",
    "    df.drop(['documents', 'concat_doc', 'num_documents'], axis=1, inplace=True)\n",
    "    df.to_csv(os.path.join('.', file[:-4] + '_mmr.csv'), index=False)\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print str(time.time() - start)\n",
    "\t\n",
    "# print ('\\nSummary:\\n')\n",
    "# for sentence in summarySet:\n",
    "# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "# print()\n",
    "\n",
    "# print '============================================================='\n",
    "# print '\\nOriginal Passages:\\n'\n",
    "# from termcolor import colored\n",
    "\n",
    "# for sentence in clean:\n",
    "# \tif sentence in summarySet:\n",
    "# \t\tprint colored(originalSentenceOf[sentence].lstrip(' '), 'red')\n",
    "# \telse:\n",
    "# \t\tprint originalSentenceOf[sentence].lstrip(' ')\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
