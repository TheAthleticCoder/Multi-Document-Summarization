{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cleanData(sentence):\n",
    "\t#sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n",
    "\t#sentence filter(None, re.split(\"[.!?\", setence))\n",
    "\tret = []\n",
    "\tsentence = stemmer.stem(sentence)\t\n",
    "\tfor word in sentence.split():\n",
    "\t\tret.append(word)\n",
    "\treturn \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorSpace(cleanSet):\n",
    "\tvocab = {}\n",
    "\tfor data in cleanSet:\n",
    "\t\tfor word in data.split():\n",
    "\t\t\tvocab[data] = 0\n",
    "\treturn vocab.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSimilarity(sentence, doc):\n",
    "\tif doc == []:\n",
    "\t\treturn 0\n",
    "\tvocab = {}\n",
    "\tfor word in sentence:\n",
    "\t\tvocab[word] = 0\n",
    "\t\n",
    "\tdocInOneSentence = '';\n",
    "\tfor t in doc:\n",
    "\t\tdocInOneSentence += (t + ' ')\n",
    "\t\tfor word in t.split():\n",
    "\t\t\tvocab[word]=0\t\n",
    "\t\n",
    "\tcv = CountVectorizer(vocabulary=vocab.keys())\n",
    "\n",
    "\tdocVector = cv.fit_transform([docInOneSentence])\n",
    "\tsentenceVector = cv.fit_transform([sentence])\n",
    "\treturn cosine_similarity(docVector, sentenceVector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x):\n",
    "    # print(len(x), len(x[:-2]))\n",
    "    x = ' '.join(x)\n",
    "    x = x.split('\\n')\n",
    "    x = list(filter(lambda s: not s == ' ', x))\n",
    "    x = list(map(lambda s: s.strip(), x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(texts, sentences, clean, originalSentenceOf):\n",
    "    for line in texts:\n",
    "        parts = line.split('.')\n",
    "        for part in parts:\n",
    "            cl = cleanData(part)\n",
    "            sentences.append(part)\n",
    "            clean.append(cl)\n",
    "            originalSentenceOf[cl] = part\t\t\n",
    "    setClean = set(clean)\n",
    "\n",
    "    return setClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Define the handler function to raise an exception\n",
    "def handler(signum, frame):\n",
    "    raise Exception(\"Function execution took too long\")\n",
    "\n",
    "# Set the signal handler\n",
    "signal.signal(signal.SIGALRM, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from termcolor import colored\n",
    "def get_mmr(doc, alpha):\n",
    "\ttry:\n",
    "\t\t# set an alarm for 60 seconds\n",
    "\t\tsignal.alarm(60)\n",
    "\t\tsentences = []\n",
    "\t\tclean = []\n",
    "\t\toriginalSentenceOf = {}\n",
    "\n",
    "\t\tcleanSet = get_sentences(doc, sentences, clean, originalSentenceOf)\n",
    "\n",
    "\t\tscores = {}\n",
    "\t\tfor data in clean:\n",
    "\t\t\ttemp_doc = cleanSet - set([data])\n",
    "\t\t\tscore = calculateSimilarity(data, list(temp_doc))\n",
    "\t\t\tscores[data] = score\n",
    "\n",
    "\t\tn = 20 * len(sentences) / 100\n",
    "\t\tsummarySet = []\n",
    "\t\twhile n > 0:\n",
    "\t\t\tmmr = {}\n",
    "\t\t\tfor sentence in scores.keys():\n",
    "\t\t\t\tif not sentence in summarySet:\n",
    "\t\t\t\t\tmmr[sentence] = alpha * scores[sentence] - (1-alpha) * calculateSimilarity(sentence, summarySet)\t\n",
    "\t\t\tif mmr == {}:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tselected = max(mmr.items(), key=operator.itemgetter(1))[0]\t\n",
    "\t\t\tsummarySet.append(selected)\n",
    "\t\t\tn -= 1\n",
    "\n",
    "\t\toriginal = [originalSentenceOf[sentence].strip() for sentence in summarySet]\n",
    "\t\t# print ('\\nSummary:\\n')\n",
    "\t\t# for sentence in summarySet:\n",
    "\t\t# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "\t\t# print()\n",
    "\n",
    "\t\t# print ('=============================================================')\n",
    "\t\t# print ('\\nOriginal Passages:\\n')\n",
    "\n",
    "\t\t# for sentence in clean:\n",
    "\t\t# \tif sentence in summarySet:\n",
    "\t\t# \t\tprint (colored(originalSentenceOf[sentence].lstrip(' '), 'red'))\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tprint (originalSentenceOf[sentence].lstrip(' '))\n",
    "\t\t\n",
    "\t\treturn original\n",
    "\texcept Exception as e:\n",
    "\t\treturn []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('ire-shshsh/mdes/multi_x_science:v0', type='dataset')\n",
    "path_to_file = artifact.get_path('test.csv').download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./artifacts/multi_x_science:v0/test.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(path_to_file)\n",
    "# df['documents'] = df['documents'].apply(lambda x: eval(x))\n",
    "# df['concat_doc'] = df['documents'].apply(lambda x: concat(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# dir_path = 'multi_news/'\n",
    "# dir_path = './'\n",
    "# files = ['sample_train.csv', 'sample_validation.csv', 'sample_test.csv']\n",
    "# files = ['test.csv']\n",
    "\n",
    "run = wandb.init(entity='ire-shshsh', project='mmr', job_type='mmr')\n",
    "\n",
    "for alpha in [0.2, 0.5, 0.8]:\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    df['abstracts'] = df['abstracts'].progress_apply(lambda x: eval(x))\n",
    "    df['concat_doc'] = df['abstracts'].progress_apply(lambda x: concat(x))\n",
    "\n",
    "    df['mmr'] = ''\n",
    "\n",
    "    # Write the header to the file\n",
    "    df.iloc[0:0].drop(['abstracts', 'concat_doc', 'cite_N'], axis=1).to_csv(f'test_{alpha}.csv', index=False)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        df.at[i, 'mmr'] = get_mmr(df.at[i, 'concat_doc'], alpha)\n",
    "        if df.at[i, 'mmr'] == []:\n",
    "            continue\n",
    "\n",
    "        row = df.iloc[i].drop(['abstracts', 'concat_doc', 'cite_N'])\n",
    "\n",
    "        # Save the current row to the file\n",
    "        row.to_frame().T.to_csv(f'test_{alpha}.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    artifact = wandb.Artifact(name=f'multi_news_test_{alpha}', type='dataset')\n",
    "    artifact.add_file(f'test_{alpha}.csv')\n",
    "    run.log_artifact(artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print str(time.time() - start)\n",
    "\t\n",
    "# print ('\\nSummary:\\n')\n",
    "# for sentence in summarySet:\n",
    "# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "# print()\n",
    "\n",
    "# print '============================================================='\n",
    "# print '\\nOriginal Passages:\\n'\n",
    "# from termcolor import colored\n",
    "\n",
    "# for sentence in clean:\n",
    "# \tif sentence in summarySet:\n",
    "# \t\tprint colored(originalSentenceOf[sentence].lstrip(' '), 'red')\n",
    "# \telse:\n",
    "# \t\tprint originalSentenceOf[sentence].lstrip(' ')\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
