{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PorterStemmer from nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create an instance of PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to clean and stem the data\n",
    "def cleanData(sentence):\n",
    "    # Use the stemmer to stem the sentence\n",
    "    return stemmer.stem(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define a function to calculate the cosine similarity between a sentence and a document\n",
    "def calculateSimilarity(sentence, doc):\n",
    "    # If the document is empty, return 0\n",
    "    if doc == []:\n",
    "        return 0\n",
    "\n",
    "    # Initialize an empty dictionary to hold the vocabulary\n",
    "    vocab = {}\n",
    "\n",
    "    # For each word in the sentence, add it to the vocabulary dictionary\n",
    "    for word in sentence:\n",
    "        vocab[word] = 0\n",
    "\n",
    "    # Initialize an empty string to hold the document in one sentence\n",
    "    docInOneSentence = ''\n",
    "\n",
    "    # For each term in the document, add it to the docInOneSentence string\n",
    "    # and add each word in the term to the vocabulary dictionary\n",
    "    for t in doc:\n",
    "        docInOneSentence += (t + ' ')\n",
    "        for word in t.split():\n",
    "            vocab[word]=0\n",
    "\n",
    "    # Initialize a CountVectorizer with the vocabulary dictionary as the vocabulary\n",
    "    cv = CountVectorizer(vocabulary=vocab.keys())\n",
    "\n",
    "    # Transform the docInOneSentence string and the sentence into vectors\n",
    "    docVector = cv.fit_transform([docInOneSentence])\n",
    "    sentenceVector = cv.fit_transform([sentence])\n",
    "\n",
    "    # Return the cosine similarity between the docVector and the sentenceVector\n",
    "    return cosine_similarity(docVector, sentenceVector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to concatenate and clean a list of strings\n",
    "def concat(x):\n",
    "    # Join all the strings in the list into one string separated by spaces\n",
    "    x = ' '.join(x)\n",
    "    \n",
    "    # Split the string into a list of strings at each newline character\n",
    "    x = x.split('\\n')\n",
    "    \n",
    "    # Filter out any strings in the list that are just a space\n",
    "    x = list(filter(lambda s: not s == ' ', x))\n",
    "    \n",
    "    # Remove leading and trailing whitespace from each string in the list\n",
    "    x = list(map(lambda s: s.strip(), x))\n",
    "    \n",
    "    # Return the cleaned list of strings\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get sentences from a text, clean them, and map them to their original form\n",
    "def get_sentences(texts, sentences, clean, originalSentenceOf):\n",
    "    # Split the text into parts at each period\n",
    "    parts = texts.split('.')\n",
    "    \n",
    "    # For each part in the parts list\n",
    "    for part in parts:\n",
    "        # Clean the part using the cleanData function\n",
    "        cl = cleanData(part)\n",
    "        \n",
    "        # Append the original part to the sentences list\n",
    "        sentences.append(part)\n",
    "        \n",
    "        # Append the cleaned part to the clean list\n",
    "        clean.append(cl)\n",
    "        \n",
    "        # Map the cleaned part to the original part in the originalSentenceOf dictionary\n",
    "        originalSentenceOf[cl] = part\n",
    "    \n",
    "    # Remove duplicates from the clean list by converting it to a set\n",
    "    setClean = set(clean)\n",
    "\n",
    "    # Return the set of cleaned parts\n",
    "    return setClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import signal\n",
    "\n",
    "# def handler(signum, frame):\n",
    "#     raise Exception(\"Function execution took too long\")\n",
    "\n",
    "# signal.signal(signal.SIGALRM, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the signal module\n",
    "import signal\n",
    "\n",
    "# Define a handler function that raises an exception when called\n",
    "def handler(signum, frame):\n",
    "    raise Exception(\"Function execution took too long\")\n",
    "\n",
    "# Set the alarm signal handler to the handler function\n",
    "# When the alarm signal is received, the handler function will be called\n",
    "signal.signal(signal.SIGALRM, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the icecream module for debugging\n",
    "from icecream import ic\n",
    "import operator\n",
    "\n",
    "# Define a function to get the Maximal Marginal Relevance (MMR)\n",
    "def get_mmr(doc, alpha):\n",
    "    try:\n",
    "        # Set an alarm for 60 seconds\n",
    "        signal.alarm(60)\n",
    "        \n",
    "        # Initialize lists and a dictionary to hold sentences, cleaned sentences, and original sentences\n",
    "        sentences = []\n",
    "        clean = []\n",
    "        originalSentenceOf = {}\n",
    "\n",
    "        # Get the set of cleaned sentences from the document\n",
    "        cleanSet = get_sentences(doc, sentences, clean, originalSentenceOf)\n",
    "\n",
    "        # Initialize a dictionary to hold the scores of the sentences\n",
    "        scores = {}\n",
    "        \n",
    "        # For each cleaned sentence, calculate its score and add it to the scores dictionary\n",
    "        for data in clean:\n",
    "            temp_doc = cleanSet - set([data])\n",
    "            score = calculateSimilarity(data, list(temp_doc))\n",
    "            scores[data] = score\n",
    "\n",
    "        # Calculate the number of sentences to include in the summary\n",
    "        n = 20 * len(sentences) / 50\n",
    "\n",
    "        # Initialize a list to hold the summary sentences\n",
    "        summarySet = []\n",
    "        \n",
    "        # While there are still sentences to add to the summary\n",
    "        while n > 0:\n",
    "            # Initialize a dictionary to hold the MMR of the sentences\n",
    "            mmr = {}\n",
    "            \n",
    "            # For each sentence, calculate its MMR and add it to the mmr dictionary\n",
    "            for sentence in scores.keys():\n",
    "                if not sentence in summarySet:\n",
    "                    mmr[sentence] = alpha * scores[sentence] - (1-alpha) * calculateSimilarity(sentence, summarySet)\t\n",
    "            \n",
    "            # If the mmr dictionary is empty, break the loop\n",
    "            if mmr == {}:\n",
    "                break\n",
    "            \n",
    "            # Select the sentence with the highest MMR and add it to the summary set\n",
    "            selected = max(mmr.items(), key=operator.itemgetter(1))[0]\t\n",
    "            summarySet.append(selected)\n",
    "            \n",
    "            # Decrease the number of sentences to add to the summary by 1\n",
    "            n -= 1\n",
    "\n",
    "        # Get the original form of the sentences in the summary set\n",
    "        original = [originalSentenceOf[sentence].strip() for sentence in summarySet]\n",
    "        \n",
    "        # Return the original sentences\n",
    "        return original\n",
    "    except Exception as e:\n",
    "        # If an exception occurs, return an empty list\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wandb module for experiment tracking\n",
    "# import wandb\n",
    "\n",
    "# Create an API object to interact with the Weights & Biases service\n",
    "# api = wandb.Api()\n",
    "\n",
    "# Get the artifact named 'ire-shshsh/mdes/multi_x_science:v0' of type 'dataset'\n",
    "# artifact = api.artifact('ire-shshsh/mdes/multi_x_science:v0', type='dataset')\n",
    "\n",
    "# Download the file 'test.csv' from the artifact and get the path to the downloaded file\n",
    "# path_to_file = artifact.get_path('test.csv').download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../our_dataset.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_file = './our_dataset - Sheet1.csv'\n",
    "path_to_file = '../our_dataset.csv'\n",
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(path_to_file)\n",
    "# df['documents'] = df['documents'].apply(lambda x: eval(x))\n",
    "# df['concat_doc'] = df['documents'].apply(lambda x: concat(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>concat_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IND vs AUS prophecy</td>\n",
       "      <td>The Cricket World Cup semi-final witnessed a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PM take on Israel-Hamas Conflict</td>\n",
       "      <td>Prime Minister Narendra Modi on Friday condemn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "0               IND vs AUS prophecy   \n",
       "1  PM take on Israel-Hamas Conflict   \n",
       "\n",
       "                                          concat_doc  \n",
       "0  The Cricket World Cup semi-final witnessed a c...  \n",
       "1  Prime Minister Narendra Modi on Friday condemn...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "\n",
    "df = pd.read_csv(path_to_file)\n",
    "df['concat_doc'] = df['doc1'] + ' ' + df['doc2'] + ' ' + df['doc3']\n",
    "df.drop(['doc1', 'doc2', 'doc3'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Initialize a Weights & Biases run\n",
    "# run = wandb.init(entity='ire-shshsh', project='mmr', job_type='mmr')\n",
    "\n",
    "# Loop over different alpha values\n",
    "for alpha in [0.2, 0.5, 0.8]:\n",
    "    # Load the data from the CSV file\n",
    "    # df = pd.read_csv(path_to_file)\n",
    "    \n",
    "    # Convert the 'abstracts' column from string to list\n",
    "    # df['abstracts'] = df['abstracts'].progress_apply(lambda x: eval(x))\n",
    "    \n",
    "    # Concatenate the abstracts in each row\n",
    "    # df['concat_doc'] = df['abstracts'].progress_apply(lambda x: concat(x))\n",
    "    \n",
    "    # Concatenate the documents in each row\n",
    "    # df['concat_doc'] = df['doc1'] + df['doc2'] + df['doc3']\n",
    "\n",
    "    # Initialize an empty 'mmr' column\n",
    "    df['mmr'] = ''\n",
    "\n",
    "    # Write the header to the file\n",
    "    df.iloc[0:0].to_csv(f'test_{alpha}.csv', index=False)\n",
    "\n",
    "    # Loop over the rows in the DataFrame\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        # Calculate the MMR for the concatenated document in the current row\n",
    "        df.at[i, 'mmr'] = get_mmr(df.at[i, 'concat_doc'], alpha)\n",
    "        \n",
    "        # If the MMR is an empty list, skip this row\n",
    "        if df.at[i, 'mmr'] == []:\n",
    "            continue\n",
    "\n",
    "        # Drop the 'concat_doc' and 'name' columns from the current row\n",
    "        row = df.iloc[i].drop(['concat_doc', 'name'])\n",
    "\n",
    "        # Save the current row to the file\n",
    "        row.to_frame().T.to_csv(f'test_{alpha}.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    # Drop the 'concat_doc' and 'name' columns from the DataFrame\n",
    "    # df.drop(['concat_doc', 'name'], axis=1, inplace=True)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    # df.to_csv(f'test_{alpha}.csv', index=False)\n",
    "    \n",
    "    # Create a Weights & Biases artifact for the CSV file\n",
    "    # artifact = wandb.Artifact(name=f'multi_news_test_{alpha}', type='dataset')\n",
    "    \n",
    "    # Add the CSV file to the artifact\n",
    "    # artifact.add_file(f'test_{alpha}.csv')\n",
    "    \n",
    "    # Log the artifact to the Weights & Biases run\n",
    "    # run.log_artifact(artifact)\n",
    "\n",
    "# Finish the Weights & Biases run\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print the time taken by the process\n",
    "# print str(time.time() - start)\n",
    "\n",
    "# Uncomment to print the summary\n",
    "# print ('\\nSummary:\\n')\n",
    "# for sentence in summarySet:\n",
    "# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "# print()\n",
    "\n",
    "# Print a separator\n",
    "# print '============================================================='\n",
    "# print '\\nOriginal Passages:\\n'\n",
    "\n",
    "# Import the termcolor module for colored output\n",
    "# from termcolor import colored\n",
    "\n",
    "# For each sentence in the cleaned data\n",
    "# for sentence in clean:\n",
    "# \t# If the sentence is in the summary set, print it in red\n",
    "# \tif sentence in summarySet:\n",
    "# \t\tprint colored(originalSentenceOf[sentence].lstrip(' '), 'red')\n",
    "# \t# Otherwise, print it in the default color\n",
    "# \telse:\n",
    "# \t\tprint originalSentenceOf[sentence].lstrip(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
