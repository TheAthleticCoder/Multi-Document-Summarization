{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def cleanData(sentence):\n",
    "\t#sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n",
    "\t#sentence filter(None, re.split(\"[.!?\", setence))\n",
    "\tret = []\n",
    "\tsentence = stemmer.stem(sentence)\t\n",
    "\tfor word in sentence.split():\n",
    "\t\tret.append(word)\n",
    "\treturn \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectorSpace(cleanSet):\n",
    "\tvocab = {}\n",
    "\tfor data in cleanSet:\n",
    "\t\tfor word in data.split():\n",
    "\t\t\tvocab[data] = 0\n",
    "\treturn vocab.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSimilarity(sentence, doc):\n",
    "\tif doc == []:\n",
    "\t\treturn 0\n",
    "\tvocab = {}\n",
    "\tfor word in sentence:\n",
    "\t\tvocab[word] = 0\n",
    "\t\n",
    "\tdocInOneSentence = '';\n",
    "\tfor t in doc:\n",
    "\t\tdocInOneSentence += (t + ' ')\n",
    "\t\tfor word in t.split():\n",
    "\t\t\tvocab[word]=0\t\n",
    "\t\n",
    "\tcv = CountVectorizer(vocabulary=vocab.keys())\n",
    "\n",
    "\tdocVector = cv.fit_transform([docInOneSentence])\n",
    "\tsentenceVector = cv.fit_transform([sentence])\n",
    "\treturn cosine_similarity(docVector, sentenceVector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(x):\n",
    "    # print(len(x), len(x[:-2]))\n",
    "    x = ' '.join(x)\n",
    "    x = x.split('\\n')\n",
    "    x = list(filter(lambda s: not s == ' ', x))\n",
    "    x = list(map(lambda s: s.strip(), x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(texts, sentences, clean, originalSentenceOf):\n",
    "    # for line in texts:\n",
    "    #     parts = line.split('.')\n",
    "    #     for part in parts:\n",
    "    #         cl = cleanData(part)\n",
    "    #         sentences.append(part)\n",
    "    #         clean.append(cl)\n",
    "    #         originalSentenceOf[cl] = part\t\t\n",
    "    parts = texts.split('.')\n",
    "    for part in parts:\n",
    "        cl = cleanData(part)\n",
    "        sentences.append(part)\n",
    "        clean.append(cl)\n",
    "        originalSentenceOf[cl] = part\t\t\n",
    "    setClean = set(clean)\n",
    "\n",
    "    return setClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.handler(signum, frame)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "# Define the handler function to raise an exception\n",
    "def handler(signum, frame):\n",
    "    raise Exception(\"Function execution took too long\")\n",
    "\n",
    "# Set the signal handler\n",
    "signal.signal(signal.SIGALRM, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from termcolor import colored\n",
    "from icecream import ic\n",
    "def get_mmr(doc, alpha):\n",
    "\ttry:\n",
    "\t\t# set an alarm for 60 seconds\n",
    "\t\tsignal.alarm(60)\n",
    "\t\tsentences = []\n",
    "\t\tclean = []\n",
    "\t\toriginalSentenceOf = {}\n",
    "\n",
    "\t\tcleanSet = get_sentences(doc, sentences, clean, originalSentenceOf)\n",
    "\n",
    "\t\tscores = {}\n",
    "\t\tfor data in clean:\n",
    "\t\t\ttemp_doc = cleanSet - set([data])\n",
    "\t\t\tscore = calculateSimilarity(data, list(temp_doc))\n",
    "\t\t\tscores[data] = score\n",
    "\n",
    "\t\tn = 20 * len(sentences) / 100\n",
    "\t\t# ic(n)\n",
    "\t\t# ic(scores.keys())\n",
    "\t\tsummarySet = []\n",
    "\t\twhile n > 0:\n",
    "\t\t\tmmr = {}\n",
    "\t\t\tfor sentence in scores.keys():\n",
    "\t\t\t\tif not sentence in summarySet:\n",
    "\t\t\t\t\tmmr[sentence] = alpha * scores[sentence] - (1-alpha) * calculateSimilarity(sentence, summarySet)\t\n",
    "\t\t\tif mmr == {}:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tselected = max(mmr.items(), key=operator.itemgetter(1))[0]\t\n",
    "\t\t\tsummarySet.append(selected)\n",
    "\t\t\tn -= 1\n",
    "\n",
    "\t\toriginal = [originalSentenceOf[sentence].strip() for sentence in summarySet]\n",
    "\t\t# print ('\\nSummary:\\n')\n",
    "\t\t# for sentence in summarySet:\n",
    "\t\t# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "\t\t# print()\n",
    "\n",
    "\t\t# print ('=============================================================')\n",
    "\t\t# print ('\\nOriginal Passages:\\n')\n",
    "\n",
    "\t\t# for sentence in clean:\n",
    "\t\t# \tif sentence in summarySet:\n",
    "\t\t# \t\tprint (colored(originalSentenceOf[sentence].lstrip(' '), 'red'))\n",
    "\t\t# \telse:\n",
    "\t\t# \t\tprint (originalSentenceOf[sentence].lstrip(' '))\n",
    "\t\t\n",
    "\t\treturn original\n",
    "\texcept Exception as e:\n",
    "\t\treturn []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# api = wandb.Api()\n",
    "# artifact = api.artifact('ire-shshsh/mdes/multi_x_science:v0', type='dataset')\n",
    "# path_to_file = artifact.get_path('test.csv').download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../our_dataset.csv'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_file = './our_dataset - Sheet1.csv'\n",
    "path_to_file = '../our_dataset.csv'\n",
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(path_to_file)\n",
    "# df['documents'] = df['documents'].apply(lambda x: eval(x))\n",
    "# df['concat_doc'] = df['documents'].apply(lambda x: concat(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>concat_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IND vs AUS prophecy</td>\n",
       "      <td>The Cricket World Cup semi-final witnessed a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PM take on Israel-Hamas Conflict</td>\n",
       "      <td>Prime Minister Narendra Modi on Friday condemn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  \\\n",
       "0               IND vs AUS prophecy   \n",
       "1  PM take on Israel-Hamas Conflict   \n",
       "\n",
       "                                          concat_doc  \n",
       "0  The Cricket World Cup semi-final witnessed a c...  \n",
       "1  Prime Minister Narendra Modi on Friday condemn...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "\n",
    "df = pd.read_csv(path_to_file)\n",
    "df['concat_doc'] = df['doc1'] + ' ' + df['doc2'] + ' ' + df['doc3']\n",
    "df.drop(['doc1', 'doc2', 'doc3'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.16it/s]\n",
      "2it [00:01,  1.17it/s]\n",
      "2it [00:01,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# dir_path = 'multi_news/'\n",
    "# dir_path = './'\n",
    "# files = ['sample_train.csv', 'sample_validation.csv', 'sample_test.csv']\n",
    "# files = ['test.csv']\n",
    "\n",
    "# run = wandb.init(entity='ire-shshsh', project='mmr', job_type='mmr')\n",
    "\n",
    "for alpha in [0.2, 0.5, 0.8]:\n",
    "    # df = pd.read_csv(path_to_file)\n",
    "    # df['abstracts'] = df['abstracts'].progress_apply(lambda x: eval(x))\n",
    "    # df['concat_doc'] = df['abstracts'].progress_apply(lambda x: concat(x))\n",
    "    # df['concat_doc'] = df['doc1'] + df['doc2'] + df['doc3']\n",
    "\n",
    "    df['mmr'] = ''\n",
    "\n",
    "    # Write the header to the file\n",
    "    df.iloc[0:0].to_csv(f'test_{alpha}.csv', index=False)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        df.at[i, 'mmr'] = get_mmr(df.at[i, 'concat_doc'], alpha)\n",
    "        if df.at[i, 'mmr'] == []:\n",
    "            continue\n",
    "\n",
    "        row = df.iloc[i].drop(['concat_doc', 'name'])\n",
    "\n",
    "        # Save the current row to the file\n",
    "        row.to_frame().T.to_csv(f'test_{alpha}.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    # df.drop(['concat_doc', 'name'], axis=1, inplace=True)\n",
    "    # df.to_csv(f'test_{alpha}.csv', index=False)\n",
    "    # artifact = wandb.Artifact(name=f'multi_news_test_{alpha}', type='dataset')\n",
    "    # artifact.add_file(f'test_{alpha}.csv')\n",
    "    # run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "#print str(time.time() - start)\n",
    "\t\n",
    "# print ('\\nSummary:\\n')\n",
    "# for sentence in summarySet:\n",
    "# \tprint (originalSentenceOf [sentence].lstrip(' '))\n",
    "# print()\n",
    "\n",
    "# print '============================================================='\n",
    "# print '\\nOriginal Passages:\\n'\n",
    "# from termcolor import colored\n",
    "\n",
    "# for sentence in clean:\n",
    "# \tif sentence in summarySet:\n",
    "# \t\tprint colored(originalSentenceOf[sentence].lstrip(' '), 'red')\n",
    "# \telse:\n",
    "# \t\tprint originalSentenceOf[sentence].lstrip(' ')\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
