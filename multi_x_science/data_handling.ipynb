{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Dataset from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit-g/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 3.94k/3.94k [00:00<00:00, 18.8MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.44k/2.44k [00:00<00:00, 8.77MB/s]\n",
      "Downloading readme: 100%|██████████| 6.39k/6.39k [00:00<00:00, 20.4MB/s]\n",
      "Downloading data: 100%|██████████| 46.1M/46.1M [00:35<00:00, 1.30MB/s]\n",
      "Downloading data: 100%|██████████| 7.60M/7.60M [00:04<00:00, 1.86MB/s]\n",
      "Downloading data: 100%|██████████| 7.68M/7.68M [00:03<00:00, 2.27MB/s]\n",
      "Generating train split: 100%|██████████| 30369/30369 [00:01<00:00, 22114.20 examples/s]\n",
      "Generating test split: 100%|██████████| 5093/5093 [00:00<00:00, 26650.19 examples/s]\n",
      "Generating validation split: 100%|██████████| 5066/5066 [00:00<00:00, 23066.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"multi_x_science_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the train, validation and test sets to pandas dataframes\n",
    "train_df = pd.DataFrame({'summary': dataset['train']['abstract'], 'ref_abstract': dataset['train']['ref_abstract']})\n",
    "validation_df = pd.DataFrame({'summary': dataset['validation']['abstract'], 'ref_abstract': dataset['validation']['ref_abstract']})\n",
    "test_df = pd.DataFrame({'summary': dataset['test']['abstract'], 'ref_abstract': dataset['test']['ref_abstract']})\n",
    "\n",
    "train_df['cite_N'] = train_df['ref_abstract'].apply(lambda x: x['cite_N'])\n",
    "train_df['abstracts'] = train_df['ref_abstract'].apply(lambda x: x['abstract'])\n",
    "train_df.drop('ref_abstract', axis=1, inplace=True)\n",
    "\n",
    "validation_df['cite_N'] = validation_df['ref_abstract'].apply(lambda x: x['cite_N'])\n",
    "validation_df['abstracts'] = validation_df['ref_abstract'].apply(lambda x: x['abstract'])\n",
    "validation_df.drop('ref_abstract', axis=1, inplace=True)\n",
    "\n",
    "test_df['cite_N'] = test_df['ref_abstract'].apply(lambda x: x['cite_N'])\n",
    "test_df['abstracts'] = test_df['ref_abstract'].apply(lambda x: x['abstract'])\n",
    "test_df.drop('ref_abstract', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30369 5066 5093\n"
     ]
    }
   ],
   "source": [
    "#original sizes of the train, validation and test sets\n",
    "print(len(train_df), len(validation_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(lst):\n",
    "    return [item for item in lst if item.strip()]\n",
    "\n",
    "train_df['abstracts'] = train_df['abstracts'].apply(remove_empty_strings)\n",
    "validation_df['abstracts'] = validation_df['abstracts'].apply(remove_empty_strings)\n",
    "test_df['abstracts'] = test_df['abstracts'].apply(remove_empty_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering to the suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22555 3773 3785\n",
      "['This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrod–Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.', 'Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.']\n",
      "2\n",
      "[\"Despite the apparent success of the Java Virtual Machine, its lackluster performance makes it ill-suited for many speed-critical applications. Although the latest just-in-time compilers and dedicated Java processors try to remedy this situation, optimized code compiled directly from a C program source is still considerably faster than software transported via Java byte-codes. This is true even if the Java byte-codes are subsequently further translated into native code. In this paper, we claim that these performance penalties are not a necessary consequence of machine-independence, but related to Java's particular intermediate representation and runtime architecture. We have constructed a prototype and are further developing a software transportability scheme founded on a tree-based alternative to Java byte-codes. This tree-based intermediate representation is not only twice as compact as Java byte-codes, but also contains more high-level information, some of which is critical for advanced code optimizations. Our architecture not only provides on-the-fly code generation from this intermediate representation, but also continuous re-optimization of the existing code-base by a low-priority background process. The re-optimization process is guided by up-to-the-minute profiling data, leading to superior runtime performance.\", 'Modifying code after the compiler has generated it can be useful for both optimization and instrumentation. Several years ago we designed the Mahler system, which uses link-time code modification for a variety of tools on our experimental Titan workstations. Killian’s Pixie tool works even later, translating a fully-linked MIPS executable file into a new version with instrumentation added. Recently we wanted to develop a hybrid of the two, that would let us experiment with both optimization and instrumentation on a standard workstation, preferably without requiring us to modify the normal compilers and linker. This paper describes prototypes of two hybrid systems, closely related to Mahler and Pixie. We implemented basic-block counting in both, and compare the resulting time and space expansion to those of Mahler and Pixie.', \"In the past few years, code optimization has become a major field of research. Many efforts have been undertaken to find new sophisticated algorithms that fully exploit the computing power of today's advanced microprocessors. Most of these algorithms do very well in statically linked, monolithic software systems, but perform perceptibly worse in extensible systems. The modular structure of these systems imposes a natural barrier for intermodular compile-time optimizations. In this paper we discuss a different approach in which optimization is no longer performed at compile-time, but is delayed until runtime. Reoptimized module versions are generated on-the-fly while the system is running, replacing earlier less optimized versions.\", 'The Smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the Smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. These features of modern programming systems are among the most difficult to implement efficiently, even individually. A new implementation of the Smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. This paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. The key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed.', 'Crossing abstraction boundaries often incurs a substantial run-time overhead in the form of frequent procedure calls. Thus, pervasive use of abstraction, while desirable from a design standpoint, may lead to very inefficient programs. Aggressively optimizing compilers can reduce this overhead but conflict with interactive programming environments because they introduce long compilation pauses and often preclude source-level debugging. Thus, programmers are caught on the horns of two dilemmas: they have to choose between abstraction and efficiency, and between responsive programming environments and efficiency. This dissertation shows how to reconcile these seemingly contradictory goals. Four new techniques work together to achieve this: - Type feedback achieves high performance by allowing the compiler to inline message sends based on information extracted from the runtime system. - Adaptive optimization achieves high responsiveness without sacrificing performance by using a fast compiler to generate initial code while automatically recompiling heavily used program parts with an optimizing compiler. - Dynamic deoptimization allows source-level debugging of optimized code by transparently recreating non-optimized code as needed. - Polymorphic inline caching speeds up message dispatch and, more significantly, collects concrete type information for the compiler. With better performance yet good interactive behavior, these techniques reconcile exploratory programming, ubiquitous abstraction, and high performance.', \"The Morph system provides a framework for automatic collection and management of profile information and application of profile-driven optimizations. In this paper, we focus on the operating system support that is required to collect and manage profile information on an end-user's workstation in an automatic, continuous, and transparent manner. Our implementation for a Digital Alpha machine running Digital UNIX 4.0 achieves run-time overheads of less than 0.3 during profile collection. Through the application of three code layout optimizations, we further show that Morph can use statistical profiles to improve application performance. With appropriate system support, automatic profiling and optimization is both possible and effective.\"]\n",
      "6\n",
      "['Polymorphic inline caches (PICs) provide a new way to reduce the overhead of polymorphic message sends by extending inline caches to include more than one cached lookup result per call site. For a set of typical object-oriented SELF programs, PICs achieve a median speedup of 11 .', 'SUMMARY This paper describes critical implementation issues that must be addressed to develop a fully automatic inliner. These issues are: integration into a compiler, program representation, hazard prevention, expansion sequence control, and program modification. An automatic inter-file inliner that uses profile information has been implemented and integrated into an optimizing C compiler. The experimental results show that this inliner achieves significant speedups for production C programs.', 'The Smalltalk-80 system makes it possible to write programs quickly by providing object-oriented programming, incremental compilation, run-time type checking, use-extensible data types and control structures, and an interactive graphical interface. However, the potential savings in programming effort have been curtailed by poor performance in widely available computers or high processor cost. Smalltalk-80 systems pose tough challenges for implementors: dynamic data typing, a high-level instruction set, frequent and expensive procedure calls, and object-oriented storage management. The dissertation documents two results that run counter to conventional wisdom: that a reduced instruction set computer can offer excellent performance for a system with dynamic data typing such as Smalltalk-80, and that automatic storage reclamation need not be time-consuming. This project was sponsored by Defense Advance Research Projects Agency (DoD) ARPA Order No. 3803, monitored by Naval Electronic System Command under Contractor No. N00034-R-0251. It was also sponsored by Defense Advance Research Projects Agency (DoD) ARPA Order No. 4871, monitored by Naval Electronic Systems Command under Contract No. N00039-84-C-0089.', \"The cost of accessing main memory is increasing. Machine designers have tried to mitigate the consequences of the processor and memory technology trends underlying this increasing gap with a variety of techniques to reduce or tolerate memory latency. These techniques, unfortunately, are only occasionally successful for pointer-manipulating programs. Recent research has demonstrated the value of a complementary approach, in which pointer-based data structures are reorganized to improve cache locality.This paper studies a technique for using a generational garbage collector to reorganize data structures to produce a cache-conscious data layout, in which objects with high temporal affinity are placed next to each other, so that they are likely to reside in the same cache block. The paper explains how to collect, with low overhead, real-time profiling information about data access patterns in object-oriented languages, and describes a new copying algorithm that utilizes this information to produce a cache-conscious object layout.Preliminary results show that this technique reduces cache miss rates by 21--42 , and improves program performance by 14--37 over Cheney's algorithm. We also compare our layouts against those produced by the Wilson-Lam-Moher algorithm, which attempts to improve program locality at the page level. Our cache-conscious object layouts reduces cache miss rates by 20--41 and improves program performance by 18--31 over their algorithm, indicating that improving locality at the page level is not necessarily beneficial at the cache level.\", 'We have developed a system called OM to explore the problem of code optimization at link-time. OM takes a collection of object modules constituting the entire program, and converts the object code into a symbolic Register Transfer Language (RTL) form that can be easily manipulated. This RTL is then transformed by intermodule optimization and finally converted back into object form. Although much high-level information about the program is gone at link-time, this approach enables us to perform optimizations that a compiler looking at a single module cannot see. Since object modules are more or less independent of the particular source language or compiler, this also gives us the chance to improve the code in ways that some compilers might simply have missed. To test the concept, we have used OM to build an optimizer that does interprocedural code motion. It moves simple loop-invariant code out of loops, even when the loop body extends across many procedures and the loop control is in a different procedure from the invariant code. Our technique also easily handles ‘‘loops’’ induced by recursion rather than iteration. Our code motion technique makes use of an interprocedural liveness analysis to discover dead registers that it can use to hold loop-invariant results. This liveness analysis also lets us perform interprocedural dead code elimination. We applied our code motion and dead code removal to SPEC benchmarks compiled with optimization using the standard compilers for the DECstation 5000. Our system improved the performance by 5 on average and by more than 14 in one case. More improvement should be possible soon; at present we move only simple load and load-address operations out of loops, and we scavenge registers to hold these values, rather than completely reallocating them. This paper will appear in the March issue of Journal of Programming Languages. It replaces Technical Note TN-31, an earlier version of the same material.', 'This paper presents the results of our investigation of code positioning techniques using execution profile data as input into the compilation process. The primary objective of the positioning is to reduce the overhead of the instruction memory hierarchy. After initial investigation in the literature, we decided to implement two prototypes for the Hewlett-Packard Precision Architecture (PA-RISC). The first, built on top of the linker, positions code based on whole procedures. This prototype has the ability to move procedures into an order that is determined by a “closest is best” strategy. The second prototype, built on top of an existing optimizer package, positions code based on basic blocks within procedures. Groups of basic blocks that would be better as straight-line sequences are identified as chains . These chains are then ordered according to branch heuristics. Code that is never executed during the data collection runs can be physically separated from the primary code of a procedure by a technique we devised called procedure splitting . The algorithms we implemented are described through examples in this paper. The performance improvements from our work are also summarized in various tables and charts.', 'A dynamic instruction trace often contains many unnecessary instructions that are required only by the unexecuted portion of the program. Hot-cold optimization (HCO) is a technique that realizes this performance opportunity. HCO uses profile information to partition each routine into frequently executed (hot) and infrequently executed (cold) parts. Unnecessary operations in the hot portion are removed, and compensation code is added on transitions from hot to cold as needed. We evaluate HCO on a collection of large Windows NT applications. HCO is most effective on the programs that are call intensive and have flat profiles, providing a 3-8 reduction in path length beyond conventional optimization.']\n",
      "7\n",
      "['This paper describes the motivations and strategies behind our group’s efforts to integrate the Tcl and Java programming languages. From the Java perspective, we wish to create a powerful scripting solution for Java applications and operating environments. From the Tcl perspective, we want to allow for cross-platform Tcl extensions and leverage the useful features and user community Java has to offer. We are specifically focusing on Java tasks like Java Bean manipulation, where a scripting solution is preferable to using straight Java code. Our goal is to create a synergy between Tcl and Java, similar to that of Visual Basic and Visual C++ on the Microsoft desktop, which makes both languages more powerful together than they are individually.', \"A mechanical brake actuator includes a manual lever which is self-locking in the active braking position. In such position, the lever and associated cable means applies tension to a spring whose force is applied to the plunger of a hydraulic master cylinder included in the conventional turntable hydraulic brake system. In the event of minor leakage and or thermal changes in the hydraulic braking system, the spring force exerted by the mechanical actuator maintains safe braking pressure when the crane is parked. When the mechanical actuator is in a release mode, the turntable hydraulic brake is foot pedal operated from the crane operator's cab without interference from the mechanical actuator.\"]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#filtering out the rows where the length of abstracts more than 1\n",
    "train_df = train_df[train_df['abstracts'].apply(len) != 1]\n",
    "train_df.reset_index(drop=True, inplace=True) #reset the index\n",
    "\n",
    "validation_df = validation_df[validation_df['abstracts'].apply(len) != 1]\n",
    "validation_df.reset_index(drop=True, inplace=True) #reset the index\n",
    "\n",
    "test_df = test_df[test_df['abstracts'].apply(len) != 1]\n",
    "test_df.reset_index(drop=True, inplace=True) #reset the index\n",
    "\n",
    "#new sizes of the train, validation and test sets\n",
    "print(len(train_df), len(validation_df), len(test_df))\n",
    "print(train_df['abstracts'][0])\n",
    "print(len(train_df['abstracts'][0]))\n",
    "print(train_df['abstracts'][1])\n",
    "print(len(train_df['abstracts'][1]))\n",
    "print(train_df['abstracts'][2])\n",
    "print(len(train_df['abstracts'][2]))\n",
    "print(train_df['abstracts'][3])\n",
    "print(len(train_df['abstracts'][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store train, validation and test in csv files\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "validation_df.to_csv('validation.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Random Summaries and Creating New Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "\n",
    "random_train = []\n",
    "random_validation = []\n",
    "random_test = []\n",
    "\n",
    "for i in train_df['abstracts']:\n",
    "    random_train.append(random.choice(i))\n",
    "    \n",
    "for i in validation_df['abstracts']:\n",
    "    random_validation.append(random.choice(i))\n",
    "    \n",
    "for i in test_df['abstracts']:\n",
    "    random_test.append(random.choice(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22555\n",
      "3773\n",
      "3785\n"
     ]
    }
   ],
   "source": [
    "modified_train_rows = []\n",
    "modified_validation_rows = []\n",
    "modified_test_rows = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    row_index = row.name #get the index of the row\n",
    "    valid_indices = [i for i in range(len(random_train)) if i != row_index]\n",
    "    random_indices = random.sample(valid_indices, 2)\n",
    "    # Extract the corresponding abstracts from the random_train list\n",
    "    random_abstracts = [random_train[i] for i in random_indices]\n",
    "    abstract_list = row['abstracts']\n",
    "    # print(abstract_list)\n",
    "    abstract_list.extend(random_abstracts)\n",
    "    modified_train_rows.append({'abstracts': abstract_list, 'summary': row['summary'], 'num_abstracts': len(abstract_list)})\n",
    "    # print(random_indices)\n",
    "    # print(abstract_list)\n",
    "    \n",
    "print(len(modified_train_rows))\n",
    "\n",
    "for _, row in validation_df.iterrows():\n",
    "    row_index = row.name #get the index of the row\n",
    "    valid_indices = [i for i in range(len(random_validation)) if i != row_index]\n",
    "    random_indices = random.sample(valid_indices, 2)\n",
    "    # Extract the corresponding abstracts from the random_validation list\n",
    "    random_abstracts = [random_validation[i] for i in random_indices]\n",
    "    abstract_list = row['abstracts']\n",
    "    abstract_list.extend(random_abstracts)\n",
    "    modified_validation_rows.append({'abstracts': abstract_list, 'summary': row['summary'], 'num_abstracts': len(abstract_list)})\n",
    "    \n",
    "print(len(modified_validation_rows))\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    row_index = row.name #get the index of the row\n",
    "    valid_indices = [i for i in range(len(random_test)) if i != row_index]\n",
    "    random_indices = random.sample(valid_indices, 2)\n",
    "    # Extract the corresponding abstracts from the random_test list\n",
    "    random_abstracts = [random_test[i] for i in random_indices]\n",
    "    abstract_list = row['abstracts']\n",
    "    abstract_list.extend(random_abstracts)\n",
    "    modified_test_rows.append({'abstracts': abstract_list, 'summary': row['summary'], 'num_abstracts': len(abstract_list)})\n",
    "    \n",
    "print(len(modified_test_rows))\n",
    "    \n",
    "modified_train_df = pd.DataFrame(modified_train_rows)\n",
    "modified_validation_df = pd.DataFrame(modified_validation_rows)\n",
    "modified_test_df = pd.DataFrame(modified_test_rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the modified DataFrames as CSV files\n",
    "modified_train_df.to_csv('modified_train.csv', index=False)\n",
    "modified_validation_df.to_csv('modified_validation.csv', index=False)\n",
    "modified_test_df.to_csv('modified_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This paper describes the motivations and strategies behind our group’s efforts to integrate the Tcl and Java programming languages. From the Java perspective, we wish to create a powerful scripting solution for Java applications and operating environments. From the Tcl perspective, we want to allow for cross-platform Tcl extensions and leverage the useful features and user community Java has to offer. We are specifically focusing on Java tasks like Java Bean manipulation, where a scripting solution is preferable to using straight Java code. Our goal is to create a synergy between Tcl and Java, similar to that of Visual Basic and Visual C++ on the Microsoft desktop, which makes both languages more powerful together than they are individually.', \"A mechanical brake actuator includes a manual lever which is self-locking in the active braking position. In such position, the lever and associated cable means applies tension to a spring whose force is applied to the plunger of a hydraulic master cylinder included in the conventional turntable hydraulic brake system. In the event of minor leakage and or thermal changes in the hydraulic braking system, the spring force exerted by the mechanical actuator maintains safe braking pressure when the crane is parked. When the mechanical actuator is in a release mode, the turntable hydraulic brake is foot pedal operated from the crane operator's cab without interference from the mechanical actuator.\", 'Neural network-based controllers arc evolved for racing simulated R C cars around several tracks of varying difficulty. The transferability of driving skills acquired when evolving for a single track is evaluated, and different ways of evolving controllers able to perform well on many different tracks are investigated, ft is further shown that such generally proficient controllers can reliably be developed into specialized controllers for individual tracks. Evolution of sensor parameters together with network weights is shown to lead to higher final fitness, but only if turned on after a general controller is developed, otherwise it hinders evolution, ft is argued that simulated car racing is a scalable and relevant testbed for evolutionary robotics research, and that the results of this research can be useful for commercial computer games.', 'In this paper we present software countermeasures specifically designed to counteract fault injection attacks during the execution of a software implementation of a cryptographic algorithm and analyze the efficiency of these countermeasures. We propose two approaches based on the insertion of redundant computations and checks, which in their general form are suitable for any cryptographic algorithm. In particular, we focus on selective instruction duplication to detect single errors, instruction triplication to support error correction, and parity checking to detect corruption of a stored value. We developed a framework to automatically add the desired countermeasure, and we support the possibility to apply the selected redundancy to either all the instructions of the cryptographic routine or restrict it to the most sensitive ones, such as table lookups and key fetching. Considering an ARM processor as a target platform and AES as a target algorithm, we evaluate the overhead of the proposed countermeasures while keeping the robustness of the implementation high enough to thwart most or all of the known fault attacks. Experimental results show that in the considered architecture, the solution with the smallest overhead is per-instruction selective doubling and checking, and that the instruction triplication scheme is a viable alternative if very high levels of injected fault resistance are required.']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "print(modified_train_df['abstracts'][num])\n",
    "print(len(modified_train_df['abstracts'][num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 250 250\n"
     ]
    }
   ],
   "source": [
    "#taking a subset of rows for each dataset\n",
    "sample_train_df = modified_train_df.sample(n=500, random_state=42)\n",
    "sample_validation_df = modified_validation_df.sample(n=250, random_state=42)\n",
    "sample_test_df = modified_test_df.sample(n=250, random_state=42)\n",
    "\n",
    "#save the sample DataFrames as CSV files\n",
    "sample_train_df.to_csv('sample_train.csv', index=False)\n",
    "sample_validation_df.to_csv('sample_validation.csv', index=False)\n",
    "sample_test_df.to_csv('sample_test.csv', index=False)\n",
    "\n",
    "print(len(sample_train_df), len(sample_validation_df), len(sample_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               abstracts  \\\n",
      "15526  [Erasure codes, such as Reed-Solomon (RS) code...   \n",
      "14187  [Most modern convolutional neural networks (CN...   \n",
      "8056   [We describe a technique for building hash ind...   \n",
      "5523   [The rapid urban expansion has greatly extende...   \n",
      "14989  [We show that if a connected graph with n node...   \n",
      "\n",
      "                                                 summary  num_abstracts  \n",
      "15526  Erasure codes offer an efficient way to decrea...              6  \n",
      "14187  This paper proposes a new method, that we call...              5  \n",
      "8056   Similarity-preserving hashing is a widely-used...              4  \n",
      "5523   In this paper, we address the problem of perso...              4  \n",
      "14989  We perform a thorough study of various charact...              8  \n"
     ]
    }
   ],
   "source": [
    "print(sample_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
